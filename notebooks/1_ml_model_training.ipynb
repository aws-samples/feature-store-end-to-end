{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "026b39c8",
   "metadata": {},
   "source": [
    "# ML Model Training\n",
    "This notebook retrieves the data from the feature store and trains an ML model using this data. The model is then deployed as a SageMaker endpoint. The model predicts a hotel cluster based on user characterestics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abba67d4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dafbab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker import get_execution_role\n",
    "import concurrent.futures\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import logging\n",
    "import psutil\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25982857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from a different path\n",
    "sys.path.insert(0, '../utils')\n",
    "path = Path(os.path.abspath(os.getcwd()))\n",
    "package_dir = f'{str(path.parent)}/utils'\n",
    "print(package_dir)\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7172927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install PyAthena if not already installed\n",
    "import pip\n",
    "def import_or_install(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        pip.main(['install', package])\n",
    "import_or_install(\"pyathena==2.3.2\")\n",
    "from pyathena import connect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec9b8ae",
   "metadata": {},
   "source": [
    "## Setup Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b736143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('__name__')\n",
    "logging.basicConfig(format=\"%(asctime)s,%(filename)s,%(funcName)s,%(lineno)s,%(levelname)s,p%(process)s,%(message)s\", level=logging.INFO)       \n",
    "logger.info(f'Using SageMaker version: {sagemaker.__version__}')\n",
    "logger.info(f'Using Pandas version: {pd.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2035733",
   "metadata": {},
   "source": [
    "## Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68033891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global constants\n",
    "STACK_NAME = \"expedia-feature-store-demo-v2\"\n",
    "RANDOM_STATE = 123\n",
    "S3_DATA_DIR = \"data\"\n",
    "LOCAL_DATA_DIR = \"../data\"\n",
    "REGION = \"us-east-1\"\n",
    "AWS_FEATURE_STORE_DATABASE = \"sagemaker_featurestore\"\n",
    "ML_MODEL_TRAINING_ROUNDS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09da86de",
   "metadata": {},
   "source": [
    "## Setup Config Variables\n",
    "Read the config variables used by this notebook from the cloud formation outputs and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc98ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read output variables from cloud formation stack, these will be used as parameters throughout\n",
    "# the code\n",
    "data_bucket_name = utils.get_cfn_stack_outputs(STACK_NAME, 'DataBucketName')\n",
    "model_bucket_name = utils.get_cfn_stack_outputs(STACK_NAME, 'MLModelBucketName')\n",
    "athena_query_results_bucket_name = utils.get_cfn_stack_outputs(STACK_NAME, 'AthenaQueryResultsBucketName')\n",
    "feature_store_bucket_name = utils.get_cfn_stack_outputs(STACK_NAME, 'FeatureStoreBucketName')\n",
    "hotel_cluster_prediction_fn_arn = utils.get_cfn_stack_outputs(STACK_NAME, 'HotelClusterPredictionFunction')\n",
    "\n",
    "logger.info(f\"data_bucket_name={data_bucket_name},\\nathena_query_results_bucket_name={athena_query_results_bucket_name},\\n\"\n",
    "            f\"model_bucket_name={model_bucket_name}\\nfeature_store_bucket_name={feature_store_bucket_name},\\n\"\n",
    "            f\"hotel_cluster_prediction_fn_arn={hotel_cluster_prediction_fn_arn}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7325b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook reads the feature group name from the file in the data folder\n",
    "customer_inputs_fg_name = utils.read_param(\"customer_inputs_fg_name\")\n",
    "destinations_fg_name = utils.read_param(\"destinations_fg_name\")\n",
    "customer_inputs_fg_table = utils.read_param(\"customer_inputs_fg_table\")\n",
    "destinations_fg_table = utils.read_param(\"destinations_fg_table\")\n",
    "raw_data_dir = utils.get_cfn_stack_parameters(STACK_NAME, 'RawDataDir')\n",
    "app_name = utils.get_cfn_stack_parameters(STACK_NAME, 'AppName')\n",
    "\n",
    "training_dataset_fname = utils.get_cfn_stack_parameters(STACK_NAME, 'TrainingDatasetFileName')\n",
    "test_dataset_fname = utils.get_cfn_stack_parameters(STACK_NAME, 'TestDatasetFileName')\n",
    "validation_dataset_fname = utils.get_cfn_stack_parameters(STACK_NAME, 'ValidationDatasetFileName')\n",
    "\n",
    "training_job_instance_type = utils.get_cfn_stack_parameters(STACK_NAME, 'TrainingJobInstanceType')\n",
    "if training_job_instance_type is None:\n",
    "    training_job_instance_type = \"ml.m5.xlarge\"\n",
    "training_job_instance_count = int(utils.get_cfn_stack_parameters(STACK_NAME, 'TrainingJobNodeInstanceCount'))\n",
    "\n",
    "model_ep_instance_type = utils.get_cfn_stack_parameters(STACK_NAME, 'ModelEndpointInstanceType')\n",
    "model_ep_instance_count = int(utils.get_cfn_stack_parameters(STACK_NAME, 'ModelEndpointInstanceCount'))\n",
    "\n",
    "customer_input_stream_name = utils.get_cfn_stack_parameters(STACK_NAME, 'CustomerInputStreamName')\n",
    "            \n",
    "logger.info(f\"customer_inputs_fg_table={customer_inputs_fg_table},\\ndestinations_fg_table={destinations_fg_table},\\n\"\n",
    "            f\"customer_inputs_fg_name={customer_inputs_fg_name},\\ndestinations_fg_name={destinations_fg_name}\\n\"\n",
    "            f\"raw_data_dir={raw_data_dir},\\ntraining_dataset_fname={training_dataset_fname},\\n\"\n",
    "            f\"test_dataset_fname={test_dataset_fname},\\nvalidation_dataset_fname=-{validation_dataset_fname}\\n\"\n",
    "            f\"training_job_instance_type={training_job_instance_type},\\ntraining_job_instance_count={training_job_instance_count},\\n\"\n",
    "            f\"model_ep_instance_type={model_ep_instance_type},\\nmodel_ep_instance_count={model_ep_instance_count},\\ncustomer_input_stream_name={customer_input_stream_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04927d06",
   "metadata": {},
   "source": [
    "## Retreve training data from the offline feature stores\n",
    "\n",
    "At this point the data needed for training the model exists in two separate feature groups, the customer inputs feature group and the destinations feature group. We will use Athena to run a SQL query to join the data and then read the results into a Pandas dataframe. We want to use Athena to do the heavy lifting of joining the large datasets rather than joining it here in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f50ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "sagemaker_client = boto_session.client(service_name='sagemaker', region_name=region)\n",
    "\n",
    "featurestore_runtime = boto_session.client(service_name='sagemaker-featurestore-runtime', region_name=region)\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity()[\"Account\"]\n",
    "\n",
    "feature_store_session = sagemaker.Session(boto_session=boto_session, \n",
    "                                          sagemaker_client=sagemaker_client, \n",
    "                                          sagemaker_featurestore_runtime_client=featurestore_runtime)\n",
    "\n",
    "logger.info(f\"role={role}, region={region}, account_id={account_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5503eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d67b49",
   "metadata": {},
   "source": [
    "Setup the SQL query for the join. We join the destination principal components with the customer inputs. We exclude out columns such as user_id and is_deleted, api_invocation_time etc that are not needed during model training.\n",
    "\n",
    "Note the \"sagemaker_featurestore\" that is the default database in which AWS keeps the feature store data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad403e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = f\"\"\"\n",
    "select \n",
    "    L.hotel_cluster,\n",
    "    L.site_name,\n",
    "    L.posa_continent,\n",
    "    L.user_location_country,\n",
    "    L.user_location_region,\n",
    "    L.user_location_city,\n",
    "    L.orig_destination_distance,\n",
    "    L.user_id,\n",
    "    L.is_mobile,\n",
    "    L.is_package,\n",
    "    L.channel,\n",
    "    L.srch_adults_cnt,\n",
    "    L.srch_children_cnt,\n",
    "    L.srch_rm_cnt,\n",
    "    L.srch_destination_id,\n",
    "    L.srch_destination_type_id,\n",
    "    L.hotel_continent,\n",
    "    L.hotel_country,\n",
    "    L.hotel_market,\n",
    "    L.duration,\n",
    "    L.days_to_trip,\n",
    "    L.start_of_trip_weekend,\n",
    "    L.end_of_trip_weekend,\n",
    "    R.pc1,\n",
    "    R.pc2,\n",
    "    R.pc3\n",
    "from (\n",
    "        \"{AWS_FEATURE_STORE_DATABASE}\".\"{customer_inputs_fg_table}\" as L\n",
    "        left join \"{AWS_FEATURE_STORE_DATABASE}\".\"{destinations_fg_table}\" as R on L.srch_destination_id = R.srch_destination_id\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923fcc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"going to run the following query using Athena -> {query_string}\")\n",
    "conn = connect(s3_staging_dir=f's3://{athena_query_results_bucket_name}/',\n",
    "               region_name=REGION)\n",
    "\n",
    "df = pd.read_sql(query_string, conn)\n",
    "logger.info(f\"results of the query are in a dataframe of shape {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a740f777",
   "metadata": {},
   "source": [
    "## ML model training\n",
    "\n",
    "At this point we are ready for ML model training. We have already excluded features we did not need for training from the Athena query so no further data preparation is required. \n",
    "\n",
    "We do a train/validation/test split and store the three datasets in S3. The Sagemaker ML model training job will retrieve the data directly from S3. We use an XGBoost container for training this model. The model is a mlti-class classification model with the hotel cluster being the target variable. All data is already available in numeric form as needed by XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4ef635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the data types of each feature, we would be converting some of the int features to categorical (object)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ffded0",
   "metadata": {},
   "source": [
    "SageMaker xgboost requires that the target column be the first column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cf51b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange columns by extracting the target column (hotel_cluster) and then adding it as the first column\n",
    "# in the dataframe\n",
    "first_column = df.pop('hotel_cluster')\n",
    "df.insert(0, 'hotel_cluster', first_column)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbaca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d41bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hotel_clusters = len(df.hotel_cluster.unique())\n",
    "logger.info(f\"there are {num_hotel_clusters} unique hotel clusters in the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98fb225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test/validation split\n",
    "# Note: numpy.split works like this: for the second param (indices_or_sections) when specified as a 1-D list  say [a,b] then first\n",
    "# split return elements from 0 to a, second split contains a to b and third split contains b to the end of the array being split\n",
    "# the df.sample with frac=1 is simply shuffling the dataset\n",
    "df_train, df_validation, df_test = np.split(df.sample(frac=1, random_state=RANDOM_STATE), [int(.7*len(df)), int(.9*len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c6c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"shape of df_train={df_train.shape}, df_validation={df_validation.shape}, df_test={df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd9645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the df_test to a data folder so that we can use this for streaming data when \n",
    "# testing real-time inference\n",
    "# exclude the pc1/2/3 columns since they will be retrieved from the online feature store\n",
    "os.makedirs(LOCAL_DATA_DIR, exist_ok=True)\n",
    "fpath = os.path.join(LOCAL_DATA_DIR, test_dataset_fname)\n",
    "cols_to_be_excluded = ['hotel_cluster', 'pc1', 'pc2', 'pc3']\n",
    "df_test.loc[:, ~df_test.columns.isin(cols_to_be_excluded)].to_csv(fpath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30727bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload dataframe to S3\n",
    "utils.upload_df_to_s3(df_train, data_bucket_name, f'{S3_DATA_DIR}/{app_name}/{training_dataset_fname}')\n",
    "utils.upload_df_to_s3(df_validation, data_bucket_name, f'{S3_DATA_DIR}/{app_name}/{validation_dataset_fname}')\n",
    "utils.upload_df_to_s3(df_test, data_bucket_name, f'{S3_DATA_DIR}/{app_name}/{test_dataset_fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aef6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"df_train shape={df_train.shape}, df_validation shape={df_validation.shape}, df_test shape={df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673984f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_train.head())\n",
    "display(df_validation.head())\n",
    "display(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3927f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_location = f's3://{data_bucket_name}/{S3_DATA_DIR}/{app_name}/train.csv'\n",
    "validation_set_location = f's3://{data_bucket_name}/{S3_DATA_DIR}/{app_name}/validation.csv'\n",
    "test_set_location = f's3://{data_bucket_name}/{S3_DATA_DIR}/{app_name}/test.csv'\n",
    "\n",
    "train_set_pointer = TrainingInput(s3_data=train_set_location, content_type='csv')\n",
    "validation_set_pointer = TrainingInput(s3_data=validation_set_location, content_type='csv')\n",
    "test_set_pointer = TrainingInput(s3_data=test_set_location, content_type='csv')\n",
    "logger.info(f\"train_set_pointer -> {json.dumps(train_set_pointer.__dict__, indent=2)},\\n\"\n",
    "            f\"validation_set_pointer -> {json.dumps(validation_set_pointer.__dict__, indent=2)},\\n\"\n",
    "            f\"test_set_pointer -> {json.dumps(test_set_pointer.__dict__, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662d0d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a short sleep to make sure that files got uploaded to S3\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b40dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_uri = sagemaker.image_uris.retrieve(region=region, \n",
    "                                              framework='xgboost', \n",
    "                                              version='1.0-1', \n",
    "                                              image_scope='training')\n",
    "job_name = f\"{app_name.replace('_', '-')}-ml-model\"\n",
    "xgb = sagemaker.estimator.Estimator(image_uri=container_uri,\n",
    "                                    role=role, \n",
    "                                    instance_count=training_job_instance_count, \n",
    "                                    instance_type=training_job_instance_type,\n",
    "                                    output_path=f's3://{data_bucket_name}/{app_name}/model-artifacts',\n",
    "                                    sagemaker_session=sagemaker_session,\n",
    "                                    base_job_name=job_name)\n",
    "\n",
    "xgb.set_hyperparameters(objective='multi:softmax',\n",
    "                        num_class=num_hotel_clusters,\n",
    "                        num_round=ML_MODEL_TRAINING_ROUNDS)\n",
    "xgb.fit({'train': train_set_pointer, 'validation': validation_set_pointer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc554fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving training job information to be used in the ML lineage module\n",
    "training_job_info = xgb.latest_training_job.describe()\n",
    "if training_job_info != None :\n",
    "    training_job_name = training_job_info[\"TrainingJobName\"]\n",
    "    utils.write_param(\"training_job_name\", training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b4ffc",
   "metadata": {},
   "source": [
    "## Host the trained model as a SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee893c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"going to deploy the trained model to model_ep_instance_type={model_ep_instance_type}, model_ep_instance_count={model_ep_instance_count}\")\n",
    "xgb_predictor = xgb.deploy(initial_instance_count=model_ep_instance_count,\n",
    "                           instance_type=model_ep_instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3a75d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model will be accepting csv as input\n",
    "csv_serializer = CSVSerializer()\n",
    "\n",
    "# store the endpoint in a filename for next stage (lineage tracking)\n",
    "endpoint_name = xgb_predictor.endpoint_name\n",
    "utils.write_param(\"endpoint_name\", endpoint_name)\n",
    "\n",
    "# setup the predictor endpoint    \n",
    "predictor = Predictor(endpoint_name=endpoint_name, \n",
    "                      serializer=csv_serializer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3272c28e",
   "metadata": {},
   "source": [
    "## Batch inference\n",
    "\n",
    "Use Python multiprocessing to get inference for the entire dataframe by first splitting it into as many dataframes as there are cores on this machine and then get predictions one row at a time for each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ae972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handy function for getting inference\n",
    "def get_inference(df):\n",
    "    # get all the dataframe content as ndarray\n",
    "    y_hat_list = []\n",
    "    for r in df.values:\n",
    "        # the first element of each row is the target variable\n",
    "        y = r[0]\n",
    "        \n",
    "        # everything from the second element onwards is a feature for that row\n",
    "        X = r[1:]\n",
    "        \n",
    "        # get the prediction. The prediction is returned as a float string so\n",
    "        # \"64.0\" for 64, so we first convert the string to float and then to int\n",
    "        # cant directly cast to int (invalid literal for int() with base 10: '' error)\n",
    "        y_hat = int(float(predictor.predict(X).decode('utf-8')))\n",
    "        \n",
    "        # append it to a list so that at the end we have a list containing predictions\n",
    "        # for each row of the input dataframe\n",
    "        y_hat_list.append(y_hat)\n",
    "    return y_hat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f57a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_procs  = psutil.cpu_count(logical=False)\n",
    "logger.info(f\"num_procs={num_procs}\")\n",
    "\n",
    "# df_test = df_test.drop('hotel_cluster_predicted', axis=1)\n",
    "# split the dataframe into as many parts as their are cores on this instance\n",
    "df_splitted = np.array_split(df_test, num_procs)\n",
    "\n",
    "# list for holding predictions for each dataframe\n",
    "y_hat_list = []\n",
    "\n",
    "start = time.time()\n",
    "# setup parallel predictions for each dataframe\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=num_procs) as executor:\n",
    "    results = [ executor.submit(get_inference, df=df) for df in df_splitted ]\n",
    "    for result in concurrent.futures.as_completed(results):\n",
    "        try:\n",
    "            y_hat_list.append(result.result())\n",
    "        except Exception as ex:\n",
    "            logger.error(str(ex))\n",
    "            pass\n",
    "end = time.time()\n",
    "logger.info(f\"PPID {os.getpid()}, all done in {round(end-start,2)}s\")\n",
    "\n",
    "# flatten out the list (remember we have a list containing predicted values for each split of the original dataframe, so at this time we have a list of lists)\n",
    "y_hat = [y_hat for y_hat_sublist in y_hat_list for y_hat in y_hat_sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60907d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the prediction as a new column to the dataframe\n",
    "df_test['hotel_cluster_predicted'] = y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a136c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many did we predict correctly?\n",
    "correct = sum(df_test.hotel_cluster == df_test.hotel_cluster_predicted)\n",
    "logger.info(f\"the model predicted {correct} correctly out of {df_test.shape[0]}, accuracy={round(100*(correct/df_test.shape[0]), 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a50dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1bac82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
