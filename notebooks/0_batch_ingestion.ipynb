{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "827fd89f",
   "metadata": {},
   "source": [
    "# Batch Ingestion\n",
    "This notebook reads the raw data from an S3 bucket, transforms it for ingestion into SageMaker Feature Store and then ingests it into an offline+online Feature Store. Refer [Official SageMaker FeatureStore documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store.html) and [Python SDK](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_featurestore.html).\n",
    "\n",
    "We create two feature groups in this notebook:\n",
    "1. An offline+online feature group for customer inputs that is used for ML model training.\n",
    "2. An offline+online feature group for the destinations features, this is used both for ML model training and real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d2ecd",
   "metadata": {},
   "source": [
    "**Note:** Please set kernel to `conda_python3` for this notebook and select instance to `ml.t3.2xlarge` as part of user inputs to the CloudFormation template."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d5b88e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9297de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import logging\n",
    "import boto3\n",
    "import time\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23c1cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from a different path\n",
    "sys.path.insert(0, '../utils')\n",
    "path = Path(os.path.abspath(os.getcwd()))\n",
    "package_dir = f'{str(path.parent)}/utils'\n",
    "print(package_dir)\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521ad08f",
   "metadata": {},
   "source": [
    "## Setup Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b00ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('__name__')\n",
    "logging.basicConfig(format=\"%(asctime)s,%(filename)s,%(funcName)s,%(lineno)s,%(levelname)s,p%(process)s,%(message)s\", level=logging.INFO)       \n",
    "logger.info(f'Using SageMaker version: {sagemaker.__version__}')\n",
    "logger.info(f'Using Pandas version: {pd.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d1531",
   "metadata": {},
   "source": [
    "## Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c1436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global constants\n",
    "STACK_NAME = \"expedia-feature-store-demo-v2\"\n",
    "\n",
    "# number of worker processes to use for batch ingesting data into feature store\n",
    "MAX_WORKERS = 8\n",
    "\n",
    "# number of principal components to keep for the destinations dataset\n",
    "PC_TO_KEEP = 3\n",
    "\n",
    "# this is a sagemaker limit\n",
    "MAX_ALLOWED_FEATURE_GROUPS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12ac27d",
   "metadata": {},
   "source": [
    "## Setup Config Variables\n",
    "Read the config variables used by this notebook from the cloud formation outputs and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577089f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read output variables from cloud formation stack, these will be used as parameters throughout\n",
    "# the code\n",
    "data_bucket_name = utils.get_cfn_stack_outputs(STACK_NAME, 'DataBucketName')\n",
    "athena_query_results_bucket_name = utils.get_cfn_stack_outputs(STACK_NAME, 'AthenaQueryResultsBucketName')\n",
    "feature_store_bucket_name = utils.get_cfn_stack_outputs(STACK_NAME, 'FeatureStoreBucketName')\n",
    "logger.info(f\"data_bucket_name={data_bucket_name},\\nathena_query_results_bucket_name={athena_query_results_bucket_name},\\nfeature_store_bucket_name={feature_store_bucket_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba709a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read params from cloud formation stack. The cloud formation stack provided a convenient\n",
    "# way to provide configuration parameters for a notebook workflow without having to use\n",
    "# parameter store or other services for providing config.\n",
    "customer_inputs_fg_name = utils.get_cfn_stack_parameters(STACK_NAME, 'CustomerInputFeatureGroupName')\n",
    "destinations_fg_name = utils.get_cfn_stack_parameters(STACK_NAME, 'DestinationsFeatureGroupName')\n",
    "app_name = utils.get_cfn_stack_parameters(STACK_NAME, 'AppName')\n",
    "\n",
    "always_recreate_fg = utils.get_cfn_stack_parameters(STACK_NAME, 'AlwaysRecreateFeatureGroup')\n",
    "always_recreate_fg = True if always_recreate_fg == \"true\" else False\n",
    "\n",
    "raw_data_dir = utils.get_cfn_stack_parameters(STACK_NAME, 'RawDataDir')\n",
    "training_dataset_fname = utils.get_cfn_stack_parameters(STACK_NAME, 'TrainingDatasetFileName')\n",
    "test_dataset_fname = utils.get_cfn_stack_parameters(STACK_NAME, 'TestDatasetFileName')\n",
    "destination_features_fname = utils.get_cfn_stack_parameters(STACK_NAME, 'DestinationFeaturesFileName')\n",
    "\n",
    "# If an existing feature group by the same name is not going to be deleted then\n",
    "# append a unique suffix to the feature group name to create a new unique feature group name\n",
    "if always_recreate_fg is False:\n",
    "    dttm = datetime.now()\n",
    "    suffix = f\"{dttm.year}-{dttm.month}-{dttm.day}-{dttm.hour}-{dttm.minute}\"\n",
    "    customer_inputs_fg_name = f\"{customer_inputs_fg_name}-{suffix}\"\n",
    "    destinations_fg_name = f\"{destinations_fg_name}-{suffix}\"\n",
    "\n",
    "# log all params debugging help\n",
    "logger.info(f\"customer_inputs_fg_name={customer_inputs_fg_name},\\ndestinations_fg_name={destinations_fg_name}\\ndestination_features_fname={destination_features_fname}\\n\"\n",
    "            f\"always_recreate_fg={always_recreate_fg},\\n\"\n",
    "            f\"raw_data_dir={raw_data_dir},\\ntraining_dataset_fname={training_dataset_fname},\\n\"\n",
    "            f\"test_dataset_fname={test_dataset_fname}, app_name={app_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f324e6c8",
   "metadata": {},
   "source": [
    "## Read raw data from S3 bucket\n",
    "The raw data exists in an S3 bucket. Note that the data upload to the S3 bucket in the raw data directory (typicall raw_data) needs to be done manually prior to running this step. The data is read directly using the Pandas read_csv method. In another version of this code, Pandas will be replaced with Pyspark.\n",
    "\n",
    "We read two datasets here:\n",
    "1. The customer inputs datasets from the train.csv file that represents customers looking up hotels via the Expedia website.\n",
    "2. The destination features dataset from destinations.csv that represents embeddings for each destination, this will be joined with the customer input dataset at the time of model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdd6936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from the bucket in a pandas dataframe, this will be ingested in the feature store\n",
    "s3a_uri = f\"s3a://{data_bucket_name}/{raw_data_dir}/{training_dataset_fname}\"\n",
    "df = pd.read_csv(s3a_uri)\n",
    "logger.info(f\"shape of the dataframe read from {s3a_uri} is {df.shape}\")\n",
    "\n",
    "# drop rows with NA\n",
    "df_customer_inputs = df.dropna()\n",
    "logger.info(f\"shape of the dataframe after dropna is {df_customer_inputs.shape}\")\n",
    "display(df_customer_inputs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94257f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from the bucket in a pandas dataframe, this will be ingested in the feature store\n",
    "s3a_uri = f\"s3a://{data_bucket_name}/{raw_data_dir}/{destination_features_fname}\"\n",
    "df_destinations = pd.read_csv(s3a_uri)\n",
    "logger.info(f\"shape of the dataframe read from {s3a_uri} is {df_destinations.shape}\")\n",
    "\n",
    "# drop rows with NA\n",
    "df_destinations = df_destinations.dropna()\n",
    "logger.info(f\"shape of the dataframe after dropna is {df_destinations.shape}\")\n",
    "display(df_destinations.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17105ae7",
   "metadata": {},
   "source": [
    "## Data Transformation for Ingesting Into Feature Store\n",
    "Before this data can be ingested into the SageMaker FeatureStore, certain transformations need to be done.\n",
    "\n",
    "1. The date_time field which will be used as \"Event Time\" need to be converted to the ISO-8601 format i.e. YYYY-MM-DDTHH:MM:SSZ.\n",
    "2. The user_id field which will be used for \"Record Identifier\" needs to be converted to string.\n",
    "3. All \"object\" type fields need to be converted to string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d098c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to datetime first\n",
    "df_customer_inputs.date_time = pd.to_datetime(df_customer_inputs.date_time)\n",
    "\n",
    "# the above returns (for example) 2015-09-03 17:09:54, change this to 2015-09-03T17:09:54Z\n",
    "# The dataset documentation does not mention the timezone of the date_time so will just assume it to be UTC.\n",
    "df_customer_inputs.date_time = df_customer_inputs.date_time.map(lambda x: x.isoformat() + 'Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c018971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert user_id to string\n",
    "df_customer_inputs.user_id = df_customer_inputs.user_id.astype(\"string\")\n",
    "\n",
    "# destination id as well since this is going to be used as a key in the feature group for the destinations data\n",
    "# and the feature group record identifier can only be a string, BUT this is not the destinations table this is\n",
    "# the customer inputs table...so what gives..well, the customer inputs and destinations would be joined at the\n",
    "# time of model training and instead of doing a cast there, let's just do it here.\n",
    "df_customer_inputs.srch_destination_id = df_customer_inputs.srch_destination_id.astype(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7c3512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep rows where is_booking == 1 because we are only concerned with events when the user actually booked a hotel and that is also what the test data contains. \n",
    "if \"is_booking\" in df_customer_inputs.columns:\n",
    "    df_customer_inputs = df_customer_inputs[df_customer_inputs.is_booking == 1]\n",
    "    logger.info(f\"after removing all is_booking != 1 rows, shape of dataframe {df_customer_inputs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae6ee81",
   "metadata": {},
   "source": [
    "### Create derived features\n",
    "These features can then be stored in the Feature Store and be used for training the model. This is the advantage of having a feature store, these derived features would now be available ready to use when we want to train an ML model, any model whether it is the one being created in this repo or for a new future use-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000002b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create derived features\n",
    "\n",
    "# duration of the trip for which the hotel booking is needed seems to be intituively important\n",
    "df_customer_inputs['duration'] = (pd.to_datetime(df_customer_inputs.srch_co, errors='coerce') - pd.to_datetime(df_customer_inputs.srch_ci, errors='coerce')).astype('timedelta64[D]')\n",
    "\n",
    "# how far is the trip from the time when the user was looking up the Expedia website\n",
    "df_customer_inputs['days_to_trip'] = (pd.to_datetime(df.srch_ci, errors='coerce') - pd.to_datetime(df_customer_inputs.date_time, errors='coerce').dt.tz_localize(None)).astype('timedelta64[D]')\n",
    "\n",
    "# is the start or end of the trip on a weekend?\n",
    "df_customer_inputs['start_of_trip_weekend'] = (pd.to_datetime(df_customer_inputs.srch_ci, errors='coerce').dt.weekday >= 5).astype(int)\n",
    "df_customer_inputs['end_of_trip_weekend'] = (pd.to_datetime(df_customer_inputs.srch_co, errors='coerce').dt.weekday >= 5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e989ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert any \"object\" type columns to string\n",
    "utils.cast_object_to_string(df_customer_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_inputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a77d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the size of the dataset to make it more manageable for this demo\n",
    "unique_user_id = list(df_customer_inputs.user_id.unique())\n",
    "num_unique_user_ids = len(unique_user_id)\n",
    "logger.info(f\"there are {len(unique_user_id)} user_ids in the dataset\")\n",
    "\n",
    "# select 1% of the unique users\n",
    "import random\n",
    "FRACTION_OF_USER_IDS_TO_KEEP = 0.01\n",
    "if FRACTION_OF_USER_IDS_TO_KEEP != 1:\n",
    "    fraction_of_unique_user_ids = random.sample(unique_user_id, int(num_unique_user_ids*FRACTION_OF_USER_IDS_TO_KEEP))\n",
    "    df_customer_inputs = df_customer_inputs[df_customer_inputs.user_id.isin(fraction_of_unique_user_ids)]\n",
    "    logger.info(f\"after filtering dataframe to keep {100*FRACTION_OF_USER_IDS_TO_KEEP}% of all user_ids, dataframe shape is {df_customer_inputs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4a6e62",
   "metadata": {},
   "source": [
    "## Initialize SageMaker and FeatureStore Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d004155",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "sagemaker_client = boto_session.client(service_name='sagemaker', region_name=region)\n",
    "\n",
    "featurestore_runtime = boto_session.client(service_name='sagemaker-featurestore-runtime', region_name=region)\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity()[\"Account\"]\n",
    "logger.info(f\"role={role}, region={region}, account_id={account_id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0014beab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature store session object\n",
    "feature_store_session = Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92f5f97",
   "metadata": {},
   "source": [
    "## Cleanup Existing FeatureGroup (if needed)\n",
    "To allow running this notebook multiple time and not create a new feature group on every run we have a config parameter which controls whether or not to delete existing feature group by the same name. If the always recreate feature group param is set to false then a new feature group is created by suffixing the current datetime to the configured feature group name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26848a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of feature groups\n",
    "fg_list = sagemaker_client.list_feature_groups()\n",
    "num_feature_groups = len(fg_list['FeatureGroupSummaries'])\n",
    "if num_feature_groups == MAX_ALLOWED_FEATURE_GROUPS:\n",
    "    logger.error(f\"number fo already existing feature groups is {num_feature_groups}, no more feature groups can be created, delete some feature groups and try again\")\n",
    "logger.info(f\"there are {num_feature_groups} feature groups\")\n",
    "logger.info(fg_list)\n",
    "# if the feature group list is not empty and always recreate feature groups is set to True then delete existing feature group\n",
    "if always_recreate_fg is True and len(fg_list['FeatureGroupSummaries']) > 0:\n",
    "    logger.warning(f\"always_recreate_fg is True, going to delete feature groups\")\n",
    "    _ = [sagemaker_client.delete_feature_group(FeatureGroupName=fg['FeatureGroupName']) for fg in fg_list['FeatureGroupSummaries'] if fg['FeatureGroupName'] in [customer_inputs_fg_name, destinations_fg_name]]\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3527457f",
   "metadata": {},
   "source": [
    "# Create Feature Group\n",
    "Create a Feature Group and then set the schema from the feature group using the existing dataframe that contains the transformed data (already amenable for ingestion into feature store.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c190e082",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group = FeatureGroup(name=customer_inputs_fg_name, sagemaker_session=feature_store_session)\n",
    "feature_group.load_feature_definitions(data_frame=df_customer_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa67742",
   "metadata": {},
   "source": [
    "This is the actual feature group creation step. Note that we usually always want to create an **online + offline feature store**. Online because we want to use it for real time predictions and offline because we want to use it for model training. While in this particular use case, a separate test dataset is provided so an online datastore is much more relevant for the tedt dataset rather than the training dataset, neverthless an offline+online datastore here does not hurt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc02cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group.create(\n",
    "    s3_uri=f\"s3://{feature_store_bucket_name}/{customer_inputs_fg_name}\",\n",
    "    record_identifier_name=\"user_id\",\n",
    "    event_time_feature_name=\"date_time\",\n",
    "    role_arn=role,\n",
    "    enable_online_store=True,\n",
    "    tags=[{'Key':'project','Value':'expedia-feature-store-demo'}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f376bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_feature_group_status(feature_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9edfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ingest features into the feature group\n",
    "# actually batch ingest the data into the feature store now\n",
    "logger.info(f\"about to begin ingestion of data into feature store, max_workers={MAX_WORKERS}\")\n",
    "feature_group.ingest(\n",
    "    data_frame=df_customer_inputs, max_workers=MAX_WORKERS, wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c170180",
   "metadata": {},
   "source": [
    "## Query ingested data from the \"Online\" feature store\n",
    "This should immediately return the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66244a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use batch-get_record\n",
    "record_identifier_values = list((df_customer_inputs.user_id.unique()))[:2]\n",
    "response=featurestore_runtime.batch_get_record(\n",
    "    Identifiers=[\n",
    "        {\"FeatureGroupName\": customer_inputs_fg_name, \"RecordIdentifiersValueAsString\": record_identifier_values}\n",
    "    ]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7315c409",
   "metadata": {},
   "source": [
    "## Query ingested data from the \"Offline\" feature store\n",
    "The offline featrure store is queried using Athena. The feature store object has an Athena query method that is used to construct a query.\n",
    "\n",
    "**Note:** It could be several minutes (upto 15) until the data is ingested and available for querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9468e06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a 1 minute sleep to wait for at least some data to show up in the offline feature store\n",
    "# time.sleep(60)\n",
    "\n",
    "# the feature group provided a convenient Athena object to query the offline feature store data\n",
    "query = feature_group.athena_query()\n",
    "customers_fg_table = query.table_name\n",
    "logger.info(f\"Athena table -> fg_table={customers_fg_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a29bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = f'SELECT * FROM \"{customers_fg_table}\" limit 10'\n",
    "output_location=f's3://{athena_query_results_bucket_name}/{customer_inputs_fg_name}/query_results/'\n",
    "logger.info(f\"going to run this query -> {query_string} and store the results in {output_location}\")\n",
    "\n",
    "# run the query\n",
    "query.run(query_string=query_string, output_location=output_location)\n",
    "\n",
    "# wait for the results\n",
    "query.wait()\n",
    "df_fg = query.as_dataframe()\n",
    "\n",
    "# results\n",
    "df_fg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1a9529",
   "metadata": {},
   "source": [
    "## Save variables for next stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d01c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write feature group names and query_string to a file, used when generating lineage\n",
    "utils.write_param(\"customer_inputs_fg_name\", customer_inputs_fg_name)\n",
    "utils.write_param(\"customer_inputs_fg_table\", customers_fg_table)\n",
    "utils.write_param(\"customer_inputs_fg_query_string\", query_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa2abd",
   "metadata": {},
   "source": [
    "## Create feature group for the destination features\n",
    "We first do PCA on the destinations features to reduce it to 3 features and then store the principal components in a separate feature group of their own.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b73c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dimensions of destination folder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# the number of principal components to keep is just set to 3 here since this is a demo\n",
    "# but in an actual production model this would be determined by examining a scree plot/variance explained rule/other critiera\n",
    "pca = PCA(n_components=PC_TO_KEEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a463274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all columns except the src_destination_id \n",
    "cols_to_use = [c for c in df_destinations.columns if c != 'srch_destination_id']\n",
    "destinations_pca = pca.fit_transform(df_destinations[cols_to_use])\n",
    "df_destinations_pca = pd.DataFrame(destinations_pca, columns=[f'pc{x}' for x in range(1, (PC_TO_KEEP+1))])\n",
    "\n",
    "# typecasting the destination id to string since this is going to be used as the record identifier in the feature store\n",
    "# which has to be a string\n",
    "df_destinations_pca[\"srch_destination_id\"] = df_destinations[\"srch_destination_id\"].astype('string')\n",
    "\n",
    "# since there is no date time associated with these features in the input dataset so just use the current datetime\n",
    "from datetime import datetime\n",
    "# datetime.utcnow().isoformat() will return something like '2022-06-07T22:08:19.399890', need to\n",
    "# trunchate it to yyyy-MM-dd'T'HH:mm:ss format to make it work with sagemaker feature store\n",
    "datetime_iso8601_now = f\"{datetime.utcnow().isoformat().split('.')[0]}Z\"\n",
    "df_destinations_pca[\"date_time\"] = datetime_iso8601_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a70dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_destinations_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32f213",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_destinations_pca['date_time'] = df_destinations_pca['date_time'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e30a551",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_destinations_pca.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ad6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group = FeatureGroup(name=destinations_fg_name, sagemaker_session=feature_store_session)\n",
    "feature_group.load_feature_definitions(data_frame=df_destinations_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a4edc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group.create(\n",
    "    s3_uri=f\"s3://{feature_store_bucket_name}/{customer_inputs_fg_name}\",\n",
    "    record_identifier_name=\"srch_destination_id\",\n",
    "    event_time_feature_name=\"date_time\",\n",
    "    role_arn=role,\n",
    "    enable_online_store=True,\n",
    "    tags=[{'Key':'AppName','Value':app_name}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a0231",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_feature_group_status(feature_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b31322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ingest features into the feature group\n",
    "# actually batch ingest the data into the feature store now\n",
    "logger.info(f\"about to begin ingestion of data into feature store, max_workers={MAX_WORKERS}\")\n",
    "feature_group.ingest(\n",
    "    data_frame=df_destinations_pca, max_workers=MAX_WORKERS, wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2f6293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use batch-get_record\n",
    "record_identifier_values = list((df_destinations_pca.srch_destination_id.unique()))[:2]\n",
    "response=featurestore_runtime.batch_get_record(\n",
    "    Identifiers=[\n",
    "        {\"FeatureGroupName\": destinations_fg_name, \"RecordIdentifiersValueAsString\": record_identifier_values}\n",
    "    ]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53161cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a 1 minute sleep to wait for at least some data to show up in the offline feature store\n",
    "# time.sleep(60)\n",
    "\n",
    "# the feature group provided a convenient Athena object to query the offline feature store data\n",
    "query = feature_group.athena_query()\n",
    "destinations_fg_table = query.table_name\n",
    "logger.info(f\"Athena table -> fg_table={destinations_fg_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa32de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = f'SELECT * FROM \"{destinations_fg_table}\" limit 10'\n",
    "utils.write_param(\"destinations_fg_table\", destinations_fg_table)\n",
    "output_location=f's3://{athena_query_results_bucket_name}/{destinations_fg_name}/query_results/'\n",
    "logger.info(f\"going to run this query -> {query_string} and store the results in {output_location}\")\n",
    "\n",
    "# run the query\n",
    "query.run(query_string=query_string, output_location=output_location)\n",
    "\n",
    "# wait for the results\n",
    "query.wait()\n",
    "df_fg = query.as_dataframe()\n",
    "\n",
    "# results\n",
    "df_fg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee000d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write feature group name to a file\n",
    "utils.write_param(\"destinations_fg_name\", destinations_fg_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
